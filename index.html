<!-- 
To do:
1. News
2. site visitors
3. arxiv
4. 年份分类
5. 奖项的比例

... -->






<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 230px;
			height: 250px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 30px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
			    width: 1000px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 750px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:14pt;
			mso-bidi-font-size:14pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 5px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12pt;
		    mso-bidi-font-size: 12pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 14pt;
		    mso-bidi-font-size: 14pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 13.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:170%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 980px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>






	<title>Xinpu Liu's Homepage</title>
	<link rel="icon" type="image/x-icon" href="imgs/favicon.ico">
</head>
<body>






	<div id="container">
		<div id="left">			
			<img width="165" height="230" src="imgs/photo_lxp.jpg">
		</div>
		<div id="right">
			<div id="name">Xinpu Liu (刘心溥) </div>
			<div id="info">

				Ph.D. Student<p>
				National University of Defense Technology (NUDT)<p>
				Email: <a href="mailto:liuxinpu@nudt.edu.cn">liuxinpu@nudt.edu.cn</a><p>	
			</div>
			<a href="https://scholar.google.com/citations?hl=zh-CN&user=FZQt5WwAAAAJ" target="_blank" rel="nofollow"><span>Google Scholar</span></a> |
			<a href="https://orcid.org/0000-0001-7382-0251" target="_blank" rel="nofollow"><span>ORCiD</span></a>  |
			<a href="https://www.researchgate.net/profile/Xinpu-Liu" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			<a href="https://github.com/xinpuliu" target="_blank" rel="nofollow"><span>Github</span></a>  |
			<a href="https://github.com/xinpuliu/xinpuliu.github.io/tree/main/LiuXinpu_cv.pdf"><span>CV</span></a></a>
			</div>







		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I'm Xinpu, a passionate and self-motivated explorer of 3D vision research.
					Currently, I'm a Ph.D. student (Jan. 2023 - ) at the National University of Defense Technology (NUDT), supervised by Profs. <a href="https://www.yulanguo.cn/">Yulan Guo</a> and <a href="https://why-scholar.github.io//">Hanyun Wang</a>.
					I received my Master and B.E. degrees from NUDT in 2022 and 2020, respectively. 
					My research interests focus on 3D Computer Vision, particularly on <b style="mso-bidi-font-weight:normal">object detection</b> and <b style="mso-bidi-font-weight:normal">point cloud processing</b>.</span></p>
				</div>








	<!-- <div class="section">
		<span class="Title"><b>News</b></span><p>
		<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		<div class="paper long"><b>
			<div class="sub-right">
			<div class="paperName"><b>	
			2023.11 | One paper on multi-frame infrared small target detection is accepted to <span style="color:red">IEEE TNNLS</span>.<br>
			2023.09 | Four papers are selected as <span style="color:red">Highly Cited Papers</span> in the latest issue of ESI Index.<br>
			2023.07 | Two papers on light field image super-resolution and pointly supervised infrared small target detection are accepted to <span style="color:red">ICCV 2023</span>.<br>
			2023.02 | One paper on pointly supervised infrared small target detection is accepted to <span style="color:red">CVPR 2023</span>.<br>
			2023.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE Stereo Image SR Challenge</a> and <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/LF-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE LF Image SR Challenge</a> at CVPR 2023.<br>	
			2022.07 | Our paper "Dense Nested Attention Network for Infrared Small Target Detection" is accepted by <span style="color:red">IEEE TIP</span>. <br>
			2022.07 | Our paper "Exploring Fine-Grained Sparsity in Convolutional Neural Networks for Efficient Inference" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.03 | Two papers on network quantization and light field depth estimation are accepted to <span style="color:red">CVPR 2022</span>.<br>
			2022.02 | Our paper "Disentangling Light Fields for Super-Resolution and Disparity Estimation" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2022" target="_blank" rel="nofollow">NTIRE Stereo Image Super-Resolution Challenge</a> at CVPR 2022.<br> 
			2021.10 | Our paper "Dense Dual-Attention Network for Light Field Image Super-Resolution" is accepted by IEEE TCSVT. [<a href="https://arxiv.org/pdf/2110.12114.pdf" target="_blank" rel="nofollow">pdf</a>]<br>				
			2021.10 | Our paper "Spatial-Angular Attention Network for Light Field Reconstruction" is accepted by <span style="color:red">IEEE TIP</span>. <br>	
			2021.07 | Our paper "Learning a Single Network for Scale-Arbitrary Super-Resolution" is accepted to <span style="color:red">ICCV 2021</span>.<br>
			2021.03 | Two papers on single image super-resolution are accepted to <span style="color:red">CVPR 2021</span>.<br>
			2020.11 | Our paper "Light Field Image Super-Resolution Using Deformable Convolution" is accepted by <span style="color:red">IEEE TIP</span>.<br>
			2020.09 | An online tutorial (120 min in Chinese) regarding our Parallax Attention Mechanism is available <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">here</a>.<br>
			2020.09 | Our paper "Parallax Attention for Unsupervised Stereo Correspondence Learning" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2020.07 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is accepted to <span style="color:red">ECCV 2020</span>.<br>	
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted to WACV 2020.<br>
			2019.03 | A large-scale dataset for stereo image super-resolution is available online at <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Flickr1024</a>. <br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>.<br><br>
			</b></div>
			</b></div>
		</b></div>
	</div> -->
	







	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications & Preprints</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DuInNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Baolin Hou, Hanyun Wang*, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Under Review</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						Paper
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/GeoCom.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Geometric attribute-guided 3D semantic instance reconstruction
					</div>
					<div class="paperName">
						Junhui Wan, <b>Xinpu Liu</b>, Lili Chen, Sheng Ao, Peng Zhang*, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Image and Graphics</b></span>, 2023.<br> 
					</div>
					<div class="paperLink">
						<a href="http://www.cjig.cn/jig/ch/reader/create_pdf.aspx?file_no=230106&flag=1&year_id=2024&quarter_id=1" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/MM2Ddet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Infrared-Visible image feature dynamic selection object detection network
					</div>
					<div class="paperName">
						Ke Xu, <b>Xinpu Liu*</b>, Hanyun Wang, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Image and Graphics</b></span>, 2023.<br> 
					</div>
					<div class="paperLink">
						<a href="http://www.cjig.cn/jig/ch/reader/download_new_edit_content.aspx?edit_id=20231109142133001&file_no=202307080000002&journal_id=jig" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/AGFA.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						AGFA-Net: Adaptive Global Feature Augmentation Network for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu*</b>, Yanxin Ma, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE Geoscience and Remote Sensing Letters</b></span>, 2022.<br> 
					</div>
					<div class="paperLink">
						<a href="https://ieeexplore.ieee.org/document/9856636/" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/KASiam.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						KASiam: Keypoints-Aligned Siamese Network for the Completion of Partial TLS Point Clouds
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Yanxin Ma*, Ke Xu, Ling Wang, Jianwei Wan
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Remote Sensing</b></span>, 2022.<br> 
					</div>
					<div class="paperLink">
						<a href="https://www.mdpi.com/2072-4292/14/15/3617/pdf?version=1659605560" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/globalKNN.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Point cloud completion by dynamic transformer with adaptive neighbourhood feature fusion
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Guoquan Xu, Ke Xu, Jianwei Wan, Yanxin Ma*
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IET Computer Vision</b></span>, 2022.<br> 
					</div>
					<div class="paperLink">
						<a href="https://ietresearch.onlinelibrary.wiley.com/doi/epdf/10.1049/cvi2.12098" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>

		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/MSPCC.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Multi-scale Transformer based point cloud completion network
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Yanxin Ma*, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Image and Graphics</b></span>, 2021.<br> 
					</div>
					<div class="paperLink">
						<a href="http://www.cjig.cn/jig/ch/reader/create_pdf.aspx?file_no=20220217&flag=1&year_id=2022&quarter_id=2" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
			
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/RDSNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Human Fall Detection Method Using Millimeter-wave Radar Based on RDSNet
					</div>
					<div class="paperName">
						Zhian Yuan, Xiaoyu Zhou, <b>Xinpu Liu</b>, Dawei Lu*, Bin Deng, Yanxin Ma
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Radars</b></span>, 2021.<br> 
					</div>
					<div class="paperLink">
						<a href="https://radars.ac.cn/article/exportPdf?id=7db873e5-ac46-48a0-aeb5-bafd093f0357" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
			
			
	
		




			
		
			<div class="section">
				<span class="Title"><b>Academic Services</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					<!-- PC Members:<br>
					<a href="https://cvlai.net/ntire/2023/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2023</a>,	<br>				
					<a href="https://data.vision.ee.ethz.ch/cvl/aim22/" target="_blank" rel="nofollow">Advances in Image Manipulation (AIM) Workshop @ ECCV 2022</a>,<br>
					<a href="https://data.vision.ee.ethz.ch/cvl/ntire22/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2022</a>,<br>					
					<br>
					Challenge Organization:<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/9201" target="_blank" rel="nofollow">The 1st Light Field Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/10047" target="_blank" rel="nofollow">The 2nd Stereo Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/1598" target="_blank" rel="nofollow">The 1st Stereo Image Super-Resolution Challenge @ NTIRE 2022</a>,<br>
					<br> -->
					Journal Reviewer:<br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank" rel="nofollow">IEEE Transactions on Circuits and Systems for Video Technology</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36" target="_blank" rel="nofollow">IEEE Transactions on Geoscience and Remote Sensing</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=8859" target="_blank" rel="nofollow">IEEE Geoscience and Remote Sensing Letters</a> <br>
					<a href="https://www.sciencedirect.com/journal/computers-and-graphics" target="_blank" rel="nofollow">Computers & Graphics</a> <br>
					<a href="https://link.springer.com/journal/371" target="_blank" rel="nofollow">The Visual Computer</a> <br>		
					<a href="https://digital-library.theiet.org/content/journals/iet-cvi" target="_blank" rel="nofollow">IET Computer Vision</a> <br>	
					<a href="https://digital-library.theiet.org/content/journals/iet-ipr" target="_blank" rel="nofollow">IET Image Processing</a> <br>				
					......<br>
					Conference Reviewer:<br>
					<a href="https://www.prcv2023.cn/" target="_blank" rel="nofollow">PRCV 2023</a><br>
					<a href="https://link.springer.com/book/10.1007/978-3-031-20497-5" target="_blank" rel="nofollow">CICAI 2022</a>,
					<a href="https://link.springer.com/book/10.1007/978-981-99-8850-1" target="_blank" rel="nofollow">CICAI 2023</a>
					<br>
					......<br>
					</b></div>
			</div>		
		
		









			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>					
					Lecture: Digital signal processing (Spring Term, 2023)<br>
					Lecture: Digital signal processing (Autumn Term, 2021)<br>
				</b></div>
			</div>
		
		







			
		

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					2023 | Outstanding Paper Award of the Journal of Image and Graphics<br>
					2023 | Excellent Master Graduates of NUDT<br>
					2023 | Second place in the 4th Aerospace Cup Innovation Competition (Top2 over 227 teams)<br>
					2023 | Outstanding Student Award of NUDT<br>
					2022 | First-class Scholarship of NUDT<br>
					2021 | First place in the 3th Aerospace Cup Innovation Competition (Top1 over 422 teams)<br>
					2021 | Qiangjun Scholarship of NUDT<br>
					2020 | The 1st Prize in the Graduate Electronic Design Competition of China (Top 2%)<br>
					2020 | Outstanding B.E. Dissertation Award of NUDT<br>
					2018 | Silver Medal Winner of the University Physics Competition<br>
					2018 | Honorable Mention Award of the Interdisciplinary Contest In Modeling (ICM)<br>
				</b></div>
			</div>		
		








			<!-- site visitors begin -->
			<!-- <div style="margin:50px 0;">
				<a href="https://clustrmaps.com/site/1bffo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=500&t=tt&d=ueKrfCS3qabq9AqNETgGVXDkjNud6pEFK3nRS1f1NxQ" /></a>
			</div> -->
			<!-- site visitors end -->
		
			<!-- Last update time begin -->
			<!-- <div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2023-07-23</p>
			</div> -->
			<!-- Last update time end -->






	</div>
	
</body>
</html>
