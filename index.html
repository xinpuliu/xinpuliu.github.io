<!-- 
To do:
1. News
2. site visitors
3. arxiv
4. 年份分类

... -->


<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
	<style>
		body{
			color: #333;
		}
		#container{
			min-width: 1000px;
			width: 1000px;
/*			overflow: auto;
*/			margin: 50px auto;padding: 30px;
			/*zoom: 1;*/
*//*			border: 1px solid #ccc;background: #fc9;color: #fff;
*/		}
		#left{
			float: left;
			width: 230px;
			height: 250px;
			margin-left: 0px;
		}
		#right{
			float: left;
			width: auto;
			margin-left: 30px;
		}
		#name{
			font-size: 22.0pt;
		    mso-bidi-font-size: 24.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		        font-weight: bold;
		}
		#info{
		    font-size: 16.0pt;
		    mso-bidi-font-size: 16.0pt;
		    font-family: Times;
		    mso-bidi-font-family: Times;
		    margin-top: 30px;
		    margin-left: 5px;
		    margin-bottom: 10px;
		    
		}
		.clear{clear:both; height: 0; line-height: 0; font-size: 0}
		.Bio{
			font-size:16.0pt;
			mso-bidi-font-size:17.0pt;
			line-height:150%;
			font-family:Times;
			mso-bidi-font-family:Lato-Regular;
			text-align: justify;
		}
		span.SpellE {
		    mso-style-name: "";
		    mso-spl-e: yes;
		}
		span.Title{
			    font-size: 22.0pt;
			    mso-bidi-font-size: 17.0pt;
			    font-family: Times;
			    mso-bidi-font-family: Lato-Regular;
			    font: bold;
			    margin-top: 10px;
			    width: 1000px;
		}
		div.section{
			padding-top: 30px;
		}
		
		div.sub-left{
			float: left;
			width: 250px;
						
		}
		div.sub-left img{
			vertical-align: middle;
			horizontal-align: middle;
			margin-top: 10px;
		}
		
		div.sub-left span{
			height: 100%;
			display: inline-block;
			vertical-align: top;
						
		}
		div.sub-right{
			float: left;
			width: 750px;			
		}
		.paper{
			overflow: auto;
			zoom:1;
			border-top: 2px ;
			padding-bottom: 0px;
			min-height: 160px;

		}
		.paperTitle{
			font-size:14pt;
			mso-bidi-font-size:14pt;
			font-family:Calibri;
			mso-bidi-font-family:Calibri;
			margin-top: 10px;
			margin-bottom: 5px;
			font-weight: bold;
		}
		.paperName{
		    font-size: 12pt;
		    mso-bidi-font-size: 12pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:160%;
		    font-style: italic;
		}		
		.paperPub{
		    font-size: 14pt;
		    mso-bidi-font-size: 14pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;		    
		    font-style: italic;
		    line-height:160%;
		}
		.paperLink{
		    font-size: 13.0pt;
		    mso-bidi-font-size: 13.0pt;		    
		    font-family: Calibri;
		    mso-bidi-font-family: Times;
		    line-height:170%;
		}
		.special{
		    margin-top: 0in;
		    margin-bottom: 0in;
		    margin-left: -.9pt;
		    margin-bottom: .0001pt;
		    text-indent: .9pt;
		    mso-pagination: none;
		    tab-stops: 13.75in;
		    mso-layout-grid-align: none;
		    text-autospace: none;
		}
		.long div.sub-left, .long div.sub-right{
			height: 300px;
			width: 980px;

		}
		.short div.sub-left, .short div.sub-right{
			height:150px;

		}
		div.sub-left,div.sub-right{
			height:200px;

		}
	</style>
	<title>Xinpu Liu's Homepage</title>
	<link rel="icon" type="image/x-icon" href="imgs/favicon.ico">
</head>
<body>
	<div id="container">
		<div id="left">			
			<img width="165" height="230" src="imgs/photo_lxp.jpg">
		</div>
		<div id="right">
			<div id="name">Xinpu Liu (刘心溥) </div>
			<div id="info">

				Ph.D. Student<p>
				National University of Defense Technology (NUDT)<p>
				Email: <a href="mailto:liuxinpu@nudt.edu.cn">liuxinpu@nudt.edu.cn</a><p>	
			</div>
			         <a href="https://www.researchgate.net/profile/Xinpu-Liu" target="_blank" rel="nofollow"><span>Research Gate</span></a>  |
			         <a href="https://github.com/xinpuliu" target="_blank" rel="nofollow"><span>Github</span></a>  |
				 <a href="https://scholar.google.com/citations?hl=zh-CN&user=FZQt5WwAAAAJ" target="_blank" rel="nofollow"><span>Google Scholar</span></a>  |
				 <a href="https://github.com/xinpuliu/xinpuliu.github.io/tree/main/cv.pdf"><span>CV</span></a></a>
			
			</div>

		<div class="clear"></div>
		<div class="section">
			<span class="Title"><b>Brief Bio</b></span><p>			
				<div class="Bio">
					I'm Xinpu, a passionate explorer of 3D vision research.
					Currently, I'm a Ph.D. student (Jan. 2023 - ) at the National University of Defense Technology (NUDT), supervised by Profs. <a href="https://www.yulanguo.cn/">Yulan Guo</a> and <a href="https://why-scholar.github.io//">Hanyun Wang</a>.
					I received my Master and B.E. degrees from NUDT in 2022 and 2020, respectively. 
					My research interests focus on 3D Computer Vision, particularly on <b style="mso-bidi-font-weight:normal">object detection</b> and <b style="mso-bidi-font-weight:normal">point cloud processing</b>.</span></p>
				</div>


	<!-- <div class="section">
		<span class="Title"><b>News</b></span><p>
		<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		<div class="paper long"><b>
			<div class="sub-right">
			<div class="paperName"><b>	
			2023.11 | One paper on multi-frame infrared small target detection is accepted to <span style="color:red">IEEE TNNLS</span>.<br>
			2023.09 | Four papers are selected as <span style="color:red">Highly Cited Papers</span> in the latest issue of ESI Index.<br>
			2023.07 | Two papers on light field image super-resolution and pointly supervised infrared small target detection are accepted to <span style="color:red">ICCV 2023</span>.<br>
			2023.02 | One paper on pointly supervised infrared small target detection is accepted to <span style="color:red">CVPR 2023</span>.<br>
			2023.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE Stereo Image SR Challenge</a> and <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/LF-Image-SR/tree/NTIRE2023" target="_blank" rel="nofollow">NTIRE LF Image SR Challenge</a> at CVPR 2023.<br>	
			2022.07 | Our paper "Dense Nested Attention Network for Infrared Small Target Detection" is accepted by <span style="color:red">IEEE TIP</span>. <br>
			2022.07 | Our paper "Exploring Fine-Grained Sparsity in Convolutional Neural Networks for Efficient Inference" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.03 | Two papers on network quantization and light field depth estimation are accepted to <span style="color:red">CVPR 2022</span>.<br>
			2022.02 | Our paper "Disentangling Light Fields for Super-Resolution and Disparity Estimation" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2022.01 | We are organizing <a href="https://github.com/The-Learning-And-Vision-Atelier-LAVA/Stereo-Image-SR/tree/NTIRE2022" target="_blank" rel="nofollow">NTIRE Stereo Image Super-Resolution Challenge</a> at CVPR 2022.<br> 
			2021.10 | Our paper "Dense Dual-Attention Network for Light Field Image Super-Resolution" is accepted by IEEE TCSVT. [<a href="https://arxiv.org/pdf/2110.12114.pdf" target="_blank" rel="nofollow">pdf</a>]<br>				
			2021.10 | Our paper "Spatial-Angular Attention Network for Light Field Reconstruction" is accepted by <span style="color:red">IEEE TIP</span>. <br>	
			2021.07 | Our paper "Learning a Single Network for Scale-Arbitrary Super-Resolution" is accepted to <span style="color:red">ICCV 2021</span>.<br>
			2021.03 | Two papers on single image super-resolution are accepted to <span style="color:red">CVPR 2021</span>.<br>
			2020.11 | Our paper "Light Field Image Super-Resolution Using Deformable Convolution" is accepted by <span style="color:red">IEEE TIP</span>.<br>
			2020.09 | An online tutorial (120 min in Chinese) regarding our Parallax Attention Mechanism is available <a href="https://www.shenlanxueyuan.com/open/course/77" target="_blank" rel="nofollow">here</a>.<br>
			2020.09 | Our paper "Parallax Attention for Unsupervised Stereo Correspondence Learning" is accepted by <span style="color:red">IEEE TPAMI</span>.<br>
			2020.07 | Our paper "Spatial-Angular Interaction for Light Field Image Super-Resolution" is accepted to <span style="color:red">ECCV 2020</span>.<br>	
			2019.12 | Our paper "DeOccNet: Learning to See Through Foreground Occlusions in Light Fields" is accepted to WACV 2020.<br>
			2019.03 | A large-scale dataset for stereo image super-resolution is available online at <a href="https://yingqianwang.github.io/Flickr1024" target="_blank" rel="nofollow">Flickr1024</a>. <br>
			2019.02 | Our paper "Learning Parallax Attention for Stereo Image Super-Resolution" is accepted to <span style="color:red">CVPR 2019</span>.<br><br>
			</b></div>
			</b></div>
		</b></div>
	</div> -->
	
	
	<div class="clear"></div>
	<div class="section">
	<span class="Title"><b>Publications / Preprints</b></span><p><p>
	<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DuInNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Baolin Hou, Hanyun Wang*, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>arXiv</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Learning_Non-Local_Spatial-Angular_Correlation_for_Light_Field_Image_Super-Resolution_ICCV_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>	
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Liang_Learning_Non-Local_Spatial-Angular_ICCV_2023_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://zhengyuliang24.github.io/EPIT/" target="_blank" rel="nofollow">Webpage</a>		
						| <a href="https://github.com/ZhengyuLiang24/EPIT" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/GeoCom.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Geometric attribute-guided 3D semantic instance reconstruction
					</div>
					<div class="paperName">
						Junhui Wan, <b>Xinpu Liu</b>, Lili Chen, Sheng Ao, Peng Zhang*, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Image and Graphics</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="http://www.cjig.cn/jig/ch/reader/create_pdf.aspx?file_no=230106&flag=1&year_id=2024&quarter_id=1" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/MM2Ddet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						Infrared-Visible image feature dynamic selection object detection network
					</div>
					<div class="paperName">
						Ke Xu, <b>Xinpu Liu*</b>, Hanyun Wang, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>Journal of Image and Graphics</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="http://www.cjig.cn/jig/ch/reader/download_new_edit_content.aspx?edit_id=20231109142133001&file_no=202307080000002&journal_id=jig" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/AGFA.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						AGFA-Net: Adaptive Global Feature Augmentation Network for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu*</b>, Yanxin Ma, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>IEEE Geoscience and Remote Sensing Letters</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="https://ieeexplore.ieee.org/document/9856636/" target="_blank" rel="nofollow">Paper</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DuInNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Baolin Hou, Hanyun Wang*, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>arXiv</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Learning_Non-Local_Spatial-Angular_Correlation_for_Light_Field_Image_Super-Resolution_ICCV_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>	
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Liang_Learning_Non-Local_Spatial-Angular_ICCV_2023_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://zhengyuliang24.github.io/EPIT/" target="_blank" rel="nofollow">Webpage</a>		
						| <a href="https://github.com/ZhengyuLiang24/EPIT" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>
		
		<div class="paper short">
				<div class="sub-left">
					<span></span>
					<img border="0" width="200" height="130" src="imgs/DuInNet.jpg">
				</div>
				<div class="sub-right">
					<div class="paperTitle">
						DuInNet: Dual-Modality Feature Interaction for Point Cloud Completion
					</div>
					<div class="paperName">
						<b>Xinpu Liu</b>, Baolin Hou, Hanyun Wang*, Ke Xu, Jianwei Wan, Yulan Guo
					</div>
					<div class="paperPub">
						<span style="color:red"> <b>arXiv</b></span>, 2024.<br> 
					</div>
					<div class="paperLink">
						<a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Liang_Learning_Non-Local_Spatial-Angular_Correlation_for_Light_Field_Image_Super-Resolution_ICCV_2023_paper.pdf" target="_blank" rel="nofollow">Paper</a>	
						| <a href="https://openaccess.thecvf.com/content/ICCV2023/supplemental/Liang_Learning_Non-Local_Spatial-Angular_ICCV_2023_supplemental.pdf" target="_blank" rel="nofollow">Supp</a>	
						| <a href="https://zhengyuliang24.github.io/EPIT/" target="_blank" rel="nofollow">Webpage</a>		
						| <a href="https://github.com/ZhengyuLiang24/EPIT" target="_blank" rel="nofollow">Code</a>
					</div>
				</div>
			</div>
	
		
			
		
<div class="section">
				<span class="Title"><b>Academic Services</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					PC Members:<br>
					<a href="https://cvlai.net/ntire/2023/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2023</a>,	<br>				
					<a href="https://data.vision.ee.ethz.ch/cvl/aim22/" target="_blank" rel="nofollow">Advances in Image Manipulation (AIM) Workshop @ ECCV 2022</a>,<br>
					<a href="https://data.vision.ee.ethz.ch/cvl/ntire22/" target="_blank" rel="nofollow">New Trends in Image Restoration and Enhancement (NTIRE) Wokshop @ CVPR 2022</a>,<br>					
					<br>
					Challenge Organization:<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/9201" target="_blank" rel="nofollow">The 1st Light Field Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/10047" target="_blank" rel="nofollow">The 2nd Stereo Image Super-Resolution Challenge @ NTIRE 2023</a>,<br>
					<a href="https://codalab.lisn.upsaclay.fr/competitions/1598" target="_blank" rel="nofollow">The 1st Stereo Image Super-Resolution Challenge @ NTIRE 2022</a>,<br>
					<br>
					Conference Reviewer:<br>
					<a href="https://cvpr2021.thecvf.com/" target="_blank" rel="nofollow">CVPR 2021</a>,
					<a href="https://cvpr2022.thecvf.com/" target="_blank" rel="nofollow"> CVPR 2022</a>,
					<a href="https://cvpr2023.thecvf.com/" target="_blank" rel="nofollow">CVPR 2023</a>,
					<a href="https://cvpr.thecvf.com/" target="_blank" rel="nofollow">CVPR 2024</a><br>		
					<a href="https://eccv2022.ecva.net/" target="_blank" rel="nofollow">ECCV 2022</a><br>
					<a href="https://iccv2021.thecvf.com/" target="_blank" rel="nofollow">ICCV 2021</a>,
					<a href="https://iccv2023.thecvf.com/" target="_blank" rel="nofollow">ICCV 2023</a><br>
					<a href="https://neurips.cc/" target="_blank" rel="nofollow">NeurIPS 2023</a><br>
					<a href="https://iclr.cc/" target="_blank" rel="nofollow">ICLR 2024</a><br>
					<a href="https://aaai-23.aaai.org/" target="_blank" rel="nofollow">AAAI 2023</a>, 
					<a href="https://aaai.org/aaai-conference/" target="_blank" rel="nofollow">AAAI 2024</a><br>
					<a href="https://2021.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2021</a>, 
					<a href="https://2022.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2022</a>,
					<a href="https://2023.acmmm.org/" target="_blank" rel="nofollow">ACM MM 2023</a><br>
					......<br>
					<br>
					Journal Reviewer:<br>
					<a href="https://www.springer.com/journal/11263" target="_blank" rel="nofollow">International Journal of Computer Vision</a> <br>					
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=83" target="_blank" rel="nofollow">IEEE Transactions on Image Processing</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6046" target="_blank" rel="nofollow">IEEE Transactions on Multimedia</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76" target="_blank" rel="nofollow">IEEE Transactions on Circuits and Systems for Video Technology</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=6745852" target="_blank" rel="nofollow">IEEE Transactions on Computational Imaging</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=36" target="_blank" rel="nofollow">IEEE Transactions on Geoscience and Remote Sensing</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=11" target="_blank" rel="nofollow">IEEE Transactions on Broadcasting</a> <br>
					<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=19" target="_blank" rel="nofollow">IEEE Transactions on Instrumentation and Measurement</a> <br>
					<a href="https://www.sciencedirect.com/journal/isprs-journal-of-photogrammetry-and-remote-sensing" target="_blank" rel="nofollow">ISPRS Journal of Photogrammetry and Remote Sensing</a> <br>
					<a href="https://www.sciencedirect.com/journal/international-journal-of-applied-earth-observation-and-geoinformation?utm_campaign=STMJ_1636705839_SC&utm_medium=SRCH&utm_source=B&dgcid=STMJ_1636705839_SC" target="_blank" rel="nofollow">International Journal of Applied Earth Observation and Geoinformation</a> <br>
					<a href="https://www.journals.elsevier.com/pattern-recognition-letters" target="_blank" rel="nofollow">Pattern Recognition Letters</a> <br>
					<a href="https://www.sciencedirect.com/journal/information-fusion" target="_blank" rel="nofollow">Information Fusion</a> <br>					
					......<br>
					<br>
					</b></div>
			</div>		
		
		
			
		

			<div class="section">
				<span class="Title"><b>Teaching Assistance</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>					
					Lecture: Optical Imaging and Detection (Spring Term, 2021)<br>
					Lecture: Optical Imaging and Detection (Autumn Term, 2020)<br>
					Lecture: Signals and Systems (Spring Term, 2020)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2019)<br>
					Lecture: Target Detection and Signal Processing (Autumn Term, 2018)<br>
				</b></div>
			</div>
		
		
			
		

			<div class="section">
				<span class="Title"><b>Awards & Honors</b></span><p>
				<hr style="BORDER-LEFT-STYLE: none; BORDER-TOP: gray 2px groove; HEIGHT: 0px; BORDER-BOTTOM-STYLE: none; BORDER-RIGHT-STYLE: none">
		
				<div class="paperName"><b>
					2023 | Excellent Doctoral Graduates of NUDT (5 over 228)<br>
					2022 | First-class Scholarship of NUDT<br>
					2021 | Outstanding Master Dissertation Award of Hunan Province<br>
					2018 | Guanghua Scholarship<br>
					2016 | Excellent Graduates of Shandong Province<br>
					2015 | The 1st Prize in the Final of China Mathematics Competitions (45 winners over 63K participants, Top 0.07%)<br>	
					2015 | National Scholarship (Ministry of Education, Top 2%)<br>								
					2014 | National Scholarship (Ministry of Education, Top 2%)<br>
					2013 | National Scholarship (Ministry of Education, Top 2%)<br>
				</b></div>
			</div>		
		
			<!-- site visitors begin -->
			<!-- <div style="margin:50px 0;">
				<a href="https://clustrmaps.com/site/1bffo" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=ffffff&w=500&t=tt&d=ueKrfCS3qabq9AqNETgGVXDkjNud6pEFK3nRS1f1NxQ" /></a>
			</div> -->
			<!-- site visitors end -->
		
			<!-- Last update time begin -->
			<!-- <div style="border-top: 3px solid #555; text-align: center;">
				<p style="color: #555;">Last updated: 2023-07-23</p>
			</div> -->
			<!-- Last update time end -->

	</div>
	
</body>
</html>
